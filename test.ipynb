{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb21df55-73b1-409f-84d6-6cdce668dbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"base\") #\"large-v3\")\n",
    "\n",
    "# load audio and pad/trim it to fit 30 seconds\n",
    "# audio = whisper.load_audio(\"audio.mp3\")\n",
    "# audio = whisper.pad_or_trim(audio)\n",
    "\n",
    "# # make log-Mel spectrogram and move to the same device as the model\n",
    "# mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
    "\n",
    "# # detect the spoken language\n",
    "# _, probs = model.detect_language(mel)\n",
    "# print(f\"Detected language: {max(probs, key=probs.get)}\")\n",
    "\n",
    "# # decode the audio\n",
    "# options = whisper.DecodingOptions()\n",
    "# result = whisper.decode(model, mel, options)\n",
    "\n",
    "# # print the recognized text\n",
    "# print(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68a4a980-75f7-480a-9ee9-e4fbcc42976a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Whisper(\n",
       "  (encoder): AudioEncoder(\n",
       "    (conv1): Conv1d(128, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (blocks): ModuleList(\n",
       "      (0-31): 32 x ResidualAttentionBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): TextDecoder(\n",
       "    (token_embedding): Embedding(51866, 1280)\n",
       "    (blocks): ModuleList(\n",
       "      (0-31): 32 x ResidualAttentionBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (cross_attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (cross_attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83449f0c-1cb0-4a73-8ef0-2f96abd4c7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: zh\n",
      "都是一个动群和一个入侵证\n"
     ]
    }
   ],
   "source": [
    "audio = whisper.load_audio(\"/home/gs/Documents/zhaozhongxiang.wav\")#eng.mp3\")#\n",
    "audio = whisper.pad_or_trim(audio)\n",
    "\n",
    "# make log-Mel spectrogram and move to the same device as the model\n",
    "mel = whisper.log_mel_spectrogram(audio, n_mels = 80).to(model.device)\n",
    "\n",
    "# detect the spoken language\n",
    "_, probs = model.detect_language(mel)\n",
    "print(f\"Detected language: {max(probs, key=probs.get)}\")\n",
    "\n",
    "# decode the audio\n",
    "options = whisper.DecodingOptions()\n",
    "result = whisper.decode(model, mel, options)\n",
    "\n",
    "# print the recognized text\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60ceab88-a25d-424e-9384-467f8b7b7c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = model.embed_audio(mel[None,...])\n",
    "# mel[None,...].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb5e0663-4bc1-4ddc-b3ba-cd35bad3c396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([80, 3000]), torch.Size([1, 1500, 512]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel.shape,emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "489b1f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gs/miniconda3/envs/t2v/lib/python3.10/site-packages/datasets/load.py:2516: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n",
      "/home/gs/miniconda3/envs/t2v/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for mozilla-foundation/common_voice_11_0 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/mozilla-foundation/common_voice_11_0\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/gs/miniconda3/envs/t2v/lib/python3.10/site-packages/datasets/load.py:2516: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n",
      "/home/gs/miniconda3/envs/t2v/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for mozilla-foundation/common_voice_11_0 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/mozilla-foundation/common_voice_11_0\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
      "        num_rows: 50\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "common_voice1 = DatasetDict()\n",
    "\n",
    "common_voice1[\"train\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"zh-CN\", split=\"train\", use_auth_token=False).select(range(1000))\n",
    "# common_voice[\"train\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"zh-CN\", split=\"train+validation\", use_auth_token=True)\n",
    "common_voice1[\"test\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"zh-CN\", split=\"test\", use_auth_token=False).select(range(50))\n",
    "\n",
    "print(common_voice1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a4a83c2d-74fe-43b4-9d29-929899cde05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['audio', 'sentence'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['audio', 'sentence'],\n",
      "        num_rows: 50\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "common_voice = common_voice1.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"])\n",
    "print(common_voice)\n",
    "# tokenizer([\"hello\"]).input_ids\n",
    "# common_voice.column_names[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9779f50a-ade3-4a03-8eb0-d2c463110e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperFeatureExtractor, WhisperTokenizer, WhisperProcessor\n",
    "from datasets import Audio\n",
    "\n",
    "model_default = \"openai/whisper-small\"\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_default, language=\"zh\")\n",
    "tokenizer = WhisperTokenizer.from_pretrained(model_default, task=\"transcribe\", language=\"zh\")\n",
    "processor = WhisperProcessor.from_pretrained(model_default, task=\"transcribe\", language=\"zh\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5b1c57df-9aa0-4f6f-9334-39e16ff68f02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9666d2aa3f9c4b5c8d2e5b1226ddedef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0204b98220144f89c51c9c39e7f6b02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "    # print(f\"\"\"** {batch}\"\"\")\n",
    "    # compute log-Mel input features from input audio array \n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "    # encode target text to label ids \n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    # print(f\"\"\"** {batch[\"labels\"]}\"\"\")\n",
    "    return batch\n",
    "\n",
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0fc2cb98-bede-4f43-bde3-5086c308e312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_features', 'labels'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_features', 'labels'],\n",
       "        num_rows: 50\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "aad8c941-5787-4d32-971d-7936cddf0ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import evaluate\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "\n",
    "metric = evaluate.load(\"wer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "00f79997-1537-4ba6-ba3f-6c06bf33d76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "model.generation_config.language = \"zh\"\n",
    "\n",
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7292293c-e05f-4311-b76f-7af2089e0213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperFeatureExtractor {\n",
       "  \"chunk_length\": 30,\n",
       "  \"feature_extractor_type\": \"WhisperFeatureExtractor\",\n",
       "  \"feature_size\": 80,\n",
       "  \"hop_length\": 160,\n",
       "  \"n_fft\": 400,\n",
       "  \"n_samples\": 480000,\n",
       "  \"nb_max_frames\": 3000,\n",
       "  \"padding_side\": \"right\",\n",
       "  \"padding_value\": 0.0,\n",
       "  \"processor_class\": \"WhisperProcessor\",\n",
       "  \"return_attention_mask\": false,\n",
       "  \"sampling_rate\": 16000\n",
       "}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from transformers import WhisperConfig, WhisperModel\n",
    "# configuration = WhisperConfig.from_pretrained(\"openai/whisper-small\", language=\"zh\")\n",
    "# model = WhisperModel(configuration)\n",
    "# model.config.forced_decoder_ids = None\n",
    "# model.config.suppress_tokens = []\n",
    "feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "970f4896-08a2-4484-b421-c2d1a42e5e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gs/miniconda3/envs/t2v/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 24:09, Epoch 15/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.027500</td>\n",
       "      <td>1.413643</td>\n",
       "      <td>82.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>1.454970</td>\n",
       "      <td>78.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [], 'begin_suppress_tokens': [220, 50257]}\n",
      "/home/gs/miniconda3/envs/t2v/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [], 'begin_suppress_tokens': [220, 50257]}\n",
      "There were missing keys in the checkpoint model loaded: ['proj_out.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=0.27762053986638785, metrics={'train_runtime': 1451.7643, 'train_samples_per_second': 11.021, 'train_steps_per_second': 0.689, 'total_flos': 4.5827361570816e+18, 'train_loss': 0.27762053986638785, 'epoch': 15.87})"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-small-zh-me\",  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=100, #500,\n",
    "    max_steps=1000, #4000,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=500, #500, #1000,\n",
    "    eval_steps=500, #500, #1000,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False, #True,\n",
    "\n",
    "\n",
    ")\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=common_voice[\"train\"],\n",
    "    eval_dataset=common_voice[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "        # language=\"zh\",\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f839f02-3659-44c6-8978-c3bca0187474",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a726eb385a1049cfa6a2b7ce0113e68b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.13559322033898\n"
     ]
    }
   ],
   "source": [
    "#evaluation\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "import torch\n",
    "from evaluate import load\n",
    "from datasets import Audio\n",
    "\n",
    "# librispeech_test_clean = load_dataset(\"librispeech_asr\", \"clean\", split=\"test\")\n",
    "\n",
    "model_pth = \"/home/gs/work/audiolm-pytorch/whisper/whisper-small-zh-me/checkpoint-500\" #(\n",
    "model_token_pth = \"openai/whisper-small\"\n",
    "# model_pth = \"openai/whisper-large\"\n",
    "is_local = True\n",
    "processor = WhisperProcessor.from_pretrained(model_token_pth)#\"openai/whisper-small\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_pth, local_files_only=is_local).to(\"cuda\")\n",
    "\n",
    "def map_to_pred(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    input_features = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], return_tensors=\"pt\").input_features\n",
    "    batch[\"reference\"] = processor.tokenizer._normalize(batch['sentence'])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predicted_ids = model.generate(input_features.to(\"cuda\"))[0]\n",
    "    transcription = processor.decode(predicted_ids)\n",
    "    batch[\"prediction\"] = processor.tokenizer._normalize(transcription)\n",
    "    return batch\n",
    "\n",
    "common_voice2 = common_voice1[\"test\"].cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "result = common_voice2.map(map_to_pred)\n",
    "\n",
    "wer = load(\"wer\")\n",
    "print(100 * wer.compute(references=result[\"reference\"], predictions=result[\"prediction\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97b5cf3d-93f8-4035-96e8-44d7ece3a9eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fall', '否'),\n",
       " ('宋朝末年年间 定厥粉岭围', '宋朝末年年间定居粉岭围'),\n",
       " ('漸漸行動不便', '渐渐行动不便'),\n",
       " ('21年去世', '二十一年去世'),\n",
       " ('他们自称 chahara', '他们自称恰哈拉'),\n",
       " ('局部干涉的例子包括有口干 眼睛干燥以及阴道干燥', '局部干涩的例子包括有口干 眼睛干燥 及阴道干燥'),\n",
       " ('嘉靖38年 登晉世第三甲 第二名', '嘉靖三十八年 登进士第三甲第二名'),\n",
       " ('这一名称一直沿用至今', '这一名称一直沿用至今'),\n",
       " ('阿列河畔背塞', '阿列河畔贝赛'),\n",
       " ('同時喬凡尼還得到包稅合同和許多民反礦的經營權', '同时乔凡尼还得到包税合同和许多明矾矿的经营权'),\n",
       " ('為了懲罰西扎城和塞爾族的結盟 盟軍在抵達後將外城燒毀', '为了惩罚西扎城和塞尔柱的结盟 盟军在抵达后将外城烧毁'),\n",
       " ('盒內盛產黃色無魚鱗的奇色魚', '河内盛产黄色无鱼鳞的鳍射鱼'),\n",
       " ('毛丙 新木 江子为张科 新木 江子属下的一个变种', '毛柄新木姜子为樟科新木姜子属下的一个变种'),\n",
       " ('大理 尼阿拉伯 中俄聯邦又有5個個案', '达利尼亚亚扎科拉市镇是俄罗斯联邦伊尔库茨克州日加洛沃区所属的一个市镇'),\n",
       " ('主要演出泰米爾與電影', '他主要演出泰米尔语电影'),\n",
       " ('i', '为每个青少年播种梦想 点燃梦想 让更多青少年敢于有梦 勇于追梦 勤于圆梦 让每个青少年都为实现中国梦增添强大青春能量'),\n",
       " ('粗體字表示為主引', '粗体字表示为主演'),\n",
       " ('他是非洲基内亚籍佛德教独立党的成员', '他是非洲畿内亚及佛德角独立党的成员'),\n",
       " ('海洋建材在美國獨立戰爭中發揮了作用', '海洋建材在美国独立战争中发挥了作用'),\n",
       " ('福岐町市位于日本兵庫縣中部的祥政區劃', '福崎町是位于日本兵库县中部的行政区划'),\n",
       " ('下行于台 须有此所', '下行月台设有厕所'),\n",
       " ('以而合況胜以来而人 可无变化图四', '耶尔河畔圣伊莱尔人口变化图示'),\n",
       " ('進入庫 科學會議室 致力於推廣用語', '印度库鲁克文学会一直致力于推广用托隆库鲁克文书写库鲁克文学'),\n",
       " ('光緒八年 在眾舉人', '光绪八年再中举人'),\n",
       " ('台灣北部地區家庭多以在農曆年前時段包潤平和是在清明期間', '台湾北部地区家庭多以在农历年前时段包润饼则是在清明期间'),\n",
       " ('蔡孫白', '蔡声白'),\n",
       " ('silence', '奥夫豪森是德国巴伐利亚州的一个市镇'),\n",
       " ('該區軍隊主要負責為公海軍隊的戰略封建隊提供評估', '该区舰队主要负责为公海舰队的战列分舰队提供屏护'),\n",
       " ('雷诺在回归的第一年比赛中以第四名的成绩完成了比赛', '雷诺在回归的第一年比赛中以第四名的成绩完成了比赛'),\n",
       " ('蘋果還有紅薯', '胤禛曾向迦陵禅师印证过佛法'),\n",
       " ('这样都可以啊', '这样都可以啊'),\n",
       " ('此原理也广泛应用于家庭之中 用于生产软水', '此原理也广泛应用于家庭之中用于生产软水'),\n",
       " ('本篇的導演是趙秀賢和梁炫希', '本片的导演是赵秀贤和梁铉锡'),\n",
       " ('you', '八'),\n",
       " ('大脊亚科是大脊科旗下三个亚科的其中之一', '大戟亚科是大戟科旗下三个亚科的其中之一'),\n",
       " ('奥特拉德诺耶农村居民点是俄罗斯联邦波罗涅日州新乌斯曼区所属的一个农村居民点',\n",
       "  '奥特拉德诺耶农村居民点是俄罗斯联邦沃罗涅日州新乌斯曼区所属的一个农村居民点'),\n",
       " ('吉內斯塔', '吉内斯塔'),\n",
       " ('第二次世界大戰後 蘇聯控制波蘭統一工人黨政權 允許教會繼續履行任務', '第二次世界大战后苏联控制的波兰统一工人党政权允许教会继续履行任务'),\n",
       " ('其名称来源于当时图书馆的创始人', '其名称来源于当时图书馆的创始人'),\n",
       " ('長期信用銀行法', '长期信用银行法'),\n",
       " ('資產調期的浮動率率主要是以倫敦同業拆放率率則是指高於基本利率的部分', '资产掉期的浮动利率主要是以伦敦同业拆放利率则是指高于基本利率的部分'),\n",
       " ('主要用户是法国陆军以及阿拉伯联合大公国', '主要用户是法国陆军以及阿拉伯联合大公国'),\n",
       " ('冥王降星', '明亡降清'),\n",
       " ('如今 該符號已變成一個表示迷因的網絡詞彙', '如今该符号已变成一个表示迷因的网络词汇'),\n",
       " ('戴七宝冠做通风光', '戴七宝冠作通身光'),\n",
       " ('最少數百名夠不到門票的影迷要求加場', '最少数百名购不到门票的影迷要求加场'),\n",
       " ('他有两个哥哥', '她有两个哥哥'),\n",
       " ('因素是因素科的二年生草本植物', '罂粟是罂粟科的二年生草本植物'),\n",
       " ('以后就会明白', '以后就会明白'),\n",
       " ('1234567', '鹰潭站扩建工程二期规划将二级三场编组场扩建为单向环到反发的三级三场')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(result[\"prediction\"],result[\"reference\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
